{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:28.091363Z",
     "start_time": "2020-09-24T19:12:28.082450Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T04:34:00.471505Z",
     "start_time": "2020-09-25T04:34:00.455599Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.tokenization import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from models.model_architecture import *\n",
    "from models.train_eval import *\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "parent_path='../Data/New_Data_15-06-2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T04:34:01.419692Z",
     "start_time": "2020-09-25T04:34:01.111008Z"
    }
   },
   "outputs": [],
   "source": [
    "annotated_df=pd.read_pickle(parent_path+'Fearspeech_data_final.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:02:18.150630Z",
     "start_time": "2020-09-24T19:02:18.097236Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T04:34:11.221918Z",
     "start_time": "2020-09-25T04:34:11.205898Z"
    }
   },
   "outputs": [],
   "source": [
    "params={'model_path':'xlm-roberta-base',\n",
    "        'max_length':128,\n",
    "        'batch_size':6,\n",
    "        'weights':[1.0,1.0],\n",
    "        'data_path':parent_path+'Fearspeech_data_final.pkl',\n",
    "        'max_sentences_per_doc':10,\n",
    "        'transformer_type':'lstm_transformer',\n",
    "        'device':'cuda',\n",
    "        'learning_rate':2e-5,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':2,\n",
    "        'epochs':5\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:03:06.862173Z",
     "start_time": "2020-09-24T19:02:19.471487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [00:36<00:00, 216.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c9e65209bca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Punyajoy_folders/works_2020/Fear-speech-analysis/models/train_eval.py\u001b[0m in \u001b[0;36mtrain_phase\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading tokenizer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"bert-base-japanese\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             )\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "train_phase(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 09:30:24.217592 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "I0924 09:30:25.342872 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=tokenizer.tokenize(\"I am goof\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=['<s>']+temp+['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:46.241374Z",
     "start_time": "2020-09-24T19:12:36.073855Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f47bba9c8e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlm-roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             )\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config=AutoConfig.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:46.242703Z",
     "start_time": "2020-09-24T19:23:51.062Z"
    }
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T04:34:52.118095Z",
     "start_time": "2020-09-25T04:34:14.012273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7db92f57ba145ffa7cdb74b412bc9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "annotated_df=pd.read_pickle(params['data_path'])\n",
    "params_preprocess={'remove_numbers': True, 'remove_emoji': True, 'remove_stop_words': False, 'tokenize': False}\n",
    "list_sents=[preprocess_sent(ele,params=params_preprocess).strip() for ele in tqdm_notebook(annotated_df['message_text'],total=len(annotated_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T04:35:30.845737Z",
     "start_time": "2020-09-25T04:35:30.829143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['मदरसे   गुरुकुल    मदरसे  गुरुकुल  कारण',\n",
       " 'प्रशासक समिति    जिस भारतीय जनता पार्टी के नेताओं ने अपना पूरा राजनीतिक जीवन दाँव पर लगाकर भारत की पवित्र भूमि में से एक अयोध्या  नगरी   जहां साक्षात तीन लोकों के स्वामी भगवान   श्री राम   का जन्म हुआ  उस पवित्र भूमि पर वर्षो से लगे एक आततायी मुस्लिम शासक बाबर द्वारा राममंदिर तोड़कर बनायी बाबरी मस्जिद रूपी कलंक को धोया   आज उसी भारतीय जनता पार्टी पर अगर कोई हिंदू होकर ये आरोप लगाए कि कि भारतीय जनता पार्टी के कारण राममंदिर निर्माण में विलंभ हो रहा है तो विश्वास मानिए एसे आरोप लगाने वाले व्यक्ति की मंदबुद्धी पर मुझे तरस आता है   अगर भारतीय जनता पार्टी अयोध्या में राममंदिर नहीं बनवाएगी तो क्या समाजवादी पार्टी बनवाएगी  जिसका मुखिया  मुलायम सिंह यादव  भरे मंच पर सीना ठोक कहते हैं कि हां मैंने मुसलमान भाईयों की खातिर कारसेवकों पर गोलियां चलवाई थीं  तो क्या आप उनके ही पुत्र अखिलेश यादव से राममंदिर निर्माण की आशा लगाए हैं जिसने अपने शासन काल में मुसलमानों के लिए गाजियाबाद में करोड़ो की लागत से एशिया के सबसे बड़े हज हाऊस का निर्माण करवाया   तो क्या बसापा की मायावती राममंदिर निर्माण करवाएगी जो भगवान राम को ही नहीं मानती हैं  जिसके समर्थक भगवान राम  कृष्ण  हनुमान  शंकर के चित्रो को जलाते उन पर थूकते हुए पाए जाते हैं जिन्होंने अपने शासन काल में स्वयं की मूर्तियों का निर्माण करवाया हजारों लाखों की संख्या में करोड़ों की लागत से हाथियों की मूर्तियां बनवाई पर किसी चौराहे पर भगवान राम की एक छोटी सी मूर्ती का ही निर्माण नहीं करवाया होगा   क्या वह कांग्रेस राममंदिर का निर्माण करवाएगीजिसके लगभग सभी नेता भगवान राम के अस्तित्व पर ही प्रश्न चिन्ह खड़े करते हैं  जिसने अपने शासनकाल में हज यात्रा पर जाने वाले मुसलमानों को तो सब्सिडी देना प्रारंभ किया ओर अमरनाथ यात्रा पर जाने वाले हिंदूओ पर टैक्स लगाया  मोदी सरकार ने पिछले वर्ष ही इस  हज सब्सिडी को समाप्त किया   क्या वह कांग्रेस राममंदिर निर्माण करवाएगी जिसने अपने शासन काल में सुप्रीम कोर्ट में रामसेतु को तोड़ने के लिए हलफनामा दिया था ओर यह पक्ष रखा था कि राम सीता हनुमान वाल्मीकि आदि किरदार काल्पनिक हैं  बाद में भारतीय जनता पार्टी के विरोध के कारण ही वह रामसेतु नहीं तोड़ पायी  क्या वह कांग्रेस राममंदिर निर्माण करवाएगी जिसने हिंदू आतंकवाद भगवा आतंकवाद जैसे शब्द गढ़े  क्या वह राहुल गांधी मंदिर निर्माण करवाएंगे जिन्होंने यह कहा था कि मंदिर जाने वाले लोग ही लड़कियों को छेड़ते हैं  उनका कहने का मतलब मस्जिद ओर चर्च जाने लोग शरीफ होते हैं    धैर्य  और  विश्वास रखिए मुझे पूर्ण दृढ विश्वास हैं उचित समय आने पर नरेन्द्र मोदी जी के शासन काल में ही भव्य राममंदिर का  निर्माण हो जाएगा  ओर विश्वास मानिए जितनी बेचैनी ओर व्याकुलता हम आपको राममंदिर निर्माण को लेकर है उतनी ही नरेन्द्र मोदी जी के ह्रदय में होगी  भारतीय जनता पार्टी नरेन्द्र मोदी जी का विरोध करने वाली विभिन्न विपक्षी पार्टियों के नेता जो हिंदू भेष में छुपे बाबर की औलादें हैं ये ना तो  कभी राममंदिर निर्माण के पक्ष में रहे हैं ओर ना ही होंगे   समस्त  हिंदू समाज से  निवेदन है इन बहरूपियों बाबर की औलादों के बहकावे में बिल्कुल भी ना आएं  उचित समय आने पर अतिशीघ्र नरेन्द्र मोदी जी के ही शासन काल में भव्य राममंदिर का निर्माण होगा   जय श्री राम',\n",
       " 'सनातन धर्म रक्षक समिति    सनातन धर्म की जय हो     \"लव जिहाद\" बढ़ानेवाली फिल्म ‘केदारनाथ पर लगाएं प्रतिबंध  जनजागृति समिति   नवम्बर  wwwazaadbharatorg   हिन्दू धर्म को नीचा दिखाने व मुस्लिम व ईसाई धर्म को अच्छा दिखाने के लिए अनेक प्रयास किये जा रहे हैं  अधिकतर फिल्मों में अगर देखा जाए तो लड़की हिन्दू होती है और लकड़ा हीरो मुसलमान ही होगा जिससे लव जिहाद को बढ़ावा मिल सके और सेकुलर लड़कियां इस लव जिहाद में फंसकर अपनी जिंदगी बर्बाद कर देती हैं फिर न ही वो अपने धर्म को अपना सकती हैं और न ही दूसरे धर्म को    अभी हाल ही में एक ऐसी ही फ़िल्म आने वाली है जिसमें हिन्दू धर्म का अपनाम करके मुस्लिम धर्म को बढ़ावा दिया गया है कुछ हिन्दू तो जागरूक हुए हैं  और इस फ़िल्म का विरोध कर रहे हैं लेकिन जबतक प्रत्येक हिन्दू का इसका विरोध नहीं करेगा तब तक मूवी  बैन नहीं होगी    बता दें कि आगामी हिन्दी फिल्म ‘केदारनाथ का नाम ‘पोस्टर ‘ट्रेलर और ‘टीजर से ही यह स्पष्ट है कि यह फिल्म हिन्दूद्रोही है  इस फिल्म में एक नई खोज की गई है कि श्रीक्षेत्र केदारनाथ में वर्ष  में आए जलप्रलय की सत्य घटना इस फिल्म की नायकनायिका के कथित प्रेमप्रकरण का विरोध करने के कारण घटी है     इस फिल्म के ‘पोस्टर्स पर ‘लव इज ए पिलग्रिमेज अर्थात ‘प्रेम तीर्थयात्रा है ऐसी\\xa0 टैगलाइन देते हुए हिन्दुआें की तीर्थयात्राआें के उद्देश्य का ही साफ तौर पर मजाक उड़ाया गया है  सर्वाधिक महत्वपूर्ण यह है कि इस फिल्म में हिन्दुआें के तीर्थस्थल पर एक घोड़ेवाले मुसलमान युवक का और अमीर परिवार की हिन्दू युवती का प्रेमप्रकरण दिखाया गया है  वर्तमान में देशभर में ‘लव जिहाद की समस्या ने कोहराम मचाया हुआ है  इस फ़िल्म के माध्यम से एक प्रकार से ‘लव जिहाद को बढ़ावा दिया गया है  यह अत्यंत निंदनीय और आपत्तिजनक है     हिन्दू जनजागृति समिति ने प्रश्न किया है कि ऐसी ही कथा मुसलमानों के प्रार्थनास्थल मक्कामदीना पर मुसलमान युवती और हिन्दू युवक की प्रेमकथा दिखाने का साहस क्या निर्माता ने किया होता  इस फिल्म का हिन्दू जनजागृति समिति ने तीव्र विरोध किया है  हिन्दू जनजागृति समिति सहित समस्त हिन्दुत्वनिष्ठ संगठनों ने मांग की है कि यह फिल्म प्रदर्शित न की जाए तथा इस फिल्म पर प्रतिबंध लगाया जाए    धार्मिक भावनाएं आहत होने के कारण श्रीक्षेत्र केदारनाथ के पुजारी और श्रद्धालुआें ने भी इस फिल्म पर प्रतिबंध लगाने की मांग की है  विवाद उत्पन्न कर प्रसिद्धि प्राप्त करना और फिल्म से कमाई करनेवाले स्वार्थी और समाजघाती उद्देश्य इस फिल्म में दिखाई देते हैं  इस विषय में पुणे शहर के तिलक चौक पर फिल्म ‘केदारनाथ के विरोध में आंदोलन किया गया  इस समय हिन्दुआें की धार्मिक भावनाएं आहत करने से संबंधित विविध नारे लगाकर तीव्र निषेध व्यक्त किया गया  हिन्दू जनजागृति समिति के प्रतिनिधि मंडल की ओर से केंद्रीय फिल्म प्रमाणन मंडल के मुख्य कार्यालय में एक निवेदन दिया गया  निवेदन में मांग की गई है कि इस फिल्म को प्रमाणपत्र नहीं दिया जाए    धर्माभिमानी हिन्दू निम्न पते पर अपना विरोध दर्ज कर रहे हैं   rsvp productions   email  inforsvpfilmcom olilaperaljrgmailcom   facebook page    twitter page      kedarnath movie facebook page    twitter account    ronnie screwvala producer    gitspictures    abhishek kapoor director    central board of film certification सेन्सॉर बोर्ड phone      इमेल  chairpersoncbfcnicin ceocbfcnicin   official azaad bharat links    blogger     youtube      facebook      twitter      instagram     google      word press     pinterest         httpsapiwhatsappcomsendphonetextकरेंमैंआपकीसमितिसेजुड़नाचाहताहूँकृपयाजानकारीउपलब्धकरवाएं  हमारी सनातन धर्मरक्षक समिति से जुड़ने हेतु ऊपर दी गयी लिंक पर जाकर जानकारी अवश्य लेंतथा देश धर्म की सेवा मे अपना अमूल्य समय देकर अन्य लोगो को धर्म के प्रति जागरूक करने मै हमारा सहयोग करें   संपर्क अवश्य करें   जनजागृति के लिऐ साझा अवश्य करें  जय श्रीराम  वन्देमातरम्',\n",
       " 'modi effect muslim family names child narendra damodardas modi',\n",
       " 'आने वाली  जून  दिन गुरुवार को निर्जला एकादशी है समस्त भारत में इस दिन मीठे जल की छबील लगेगी  और \"हमदर्द\" कम्पनी का मुसलमान मालिक बहुत खुश होगा  क्योंकि हमारे हिन्दू  भाई एक दिन में ही  करोड की रूहअफजा बड़े चाव से पी जाएँगे ये जाने बिना के \"हमदर्द\" वही कंपनी है जो आज भी किसी गैर मुस्लिम को नौकरी नहीं देती है और हम ही उनकी कंपनी का रूह अफज़ा खरीद खरीद कर उनकी कंपनी का पालन पोषण करते है और यही लोग एक तरफ हमारी गाये माता का निर्दयता से कत्ल कर लाल खून बहाते है दूसरी तरफ हमें लाल रंग वाला मीठा शर्बत पिला पिला मुनाफा भी हमसे कमाते हैं और हो सकता है इसमे मे ऐसा कुछ मिलाया गया हो जो हमारे हिन्दूत्व को नष्ट करता हो अभी भी वक्त है हिन्दू भाईओं जागो और बदलो   हिंदुओं इस बार पतंजलि या डाबर के शर्बत की बोतल से शर्बत बनाओ        जागो हिन्दूओ जागो  इस पोस्ट को अभी से ही वायरल करके हिन्दओ को सावधान करे मुस्लिम समाज तो कई जगह कह चुका है कि हिन्दू दुकान से सामान मत खरीदो']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.sample(list_sents,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=[]\n",
    "for sent in list_sents:\n",
    "    count.append(len(tokenizer.tokenize(sent)))\n",
    "\n",
    "#X_0 = np.array(list_sents,dtype='object')\n",
    "# y_0 = np.array(annotated_df['one_fear_speech'])\n",
    "# tokenizer = AutoTokenizer.from_pretrained(params['model_path'])\n",
    "    \n",
    "# X_train, X_train_length = encode_documents(X_0,params,tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_greater=0\n",
    "for ele in count:\n",
    "    if(ele > (128*5)):\n",
    "        count_greater+=1\n",
    "count_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 85,\n",
       " 392,\n",
       " 42,\n",
       " 1154,\n",
       " 1245,\n",
       " 2141,\n",
       " 153,\n",
       " 340,\n",
       " 985,\n",
       " 246,\n",
       " 30,\n",
       " 209,\n",
       " 570,\n",
       " 542,\n",
       " 1067,\n",
       " 837,\n",
       " 594,\n",
       " 869,\n",
       " 1014,\n",
       " 895,\n",
       " 49,\n",
       " 667,\n",
       " 1035,\n",
       " 3217,\n",
       " 45,\n",
       " 86,\n",
       " 666,\n",
       " 1395,\n",
       " 311,\n",
       " 33,\n",
       " 137,\n",
       " 627,\n",
       " 93,\n",
       " 52,\n",
       " 210,\n",
       " 5030,\n",
       " 539,\n",
       " 1452,\n",
       " 3111,\n",
       " 588,\n",
       " 1052,\n",
       " 86,\n",
       " 65,\n",
       " 44,\n",
       " 478,\n",
       " 43,\n",
       " 1478,\n",
       " 491,\n",
       " 1261,\n",
       " 269,\n",
       " 2313,\n",
       " 308,\n",
       " 1163,\n",
       " 1262,\n",
       " 402,\n",
       " 1441,\n",
       " 322,\n",
       " 883,\n",
       " 143,\n",
       " 265,\n",
       " 415,\n",
       " 494,\n",
       " 459,\n",
       " 1012,\n",
       " 608,\n",
       " 1423,\n",
       " 1275,\n",
       " 201,\n",
       " 534,\n",
       " 890,\n",
       " 784,\n",
       " 438,\n",
       " 389,\n",
       " 887,\n",
       " 634,\n",
       " 1419,\n",
       " 123,\n",
       " 311,\n",
       " 135,\n",
       " 1622,\n",
       " 432,\n",
       " 776,\n",
       " 1189,\n",
       " 805,\n",
       " 876,\n",
       " 819,\n",
       " 250,\n",
       " 480,\n",
       " 428,\n",
       " 946,\n",
       " 832,\n",
       " 884,\n",
       " 85,\n",
       " 197,\n",
       " 44,\n",
       " 138,\n",
       " 1726,\n",
       " 646,\n",
       " 1715,\n",
       " 323,\n",
       " 3072,\n",
       " 3307,\n",
       " 202,\n",
       " 60,\n",
       " 645,\n",
       " 1178,\n",
       " 229,\n",
       " 858,\n",
       " 584,\n",
       " 2242,\n",
       " 2228,\n",
       " 53,\n",
       " 640,\n",
       " 1551,\n",
       " 264,\n",
       " 596,\n",
       " 221,\n",
       " 2835,\n",
       " 975,\n",
       " 940,\n",
       " 200,\n",
       " 351,\n",
       " 788,\n",
       " 47,\n",
       " 2251,\n",
       " 74,\n",
       " 450,\n",
       " 1633,\n",
       " 396,\n",
       " 89,\n",
       " 789,\n",
       " 53,\n",
       " 178,\n",
       " 667,\n",
       " 962,\n",
       " 42,\n",
       " 30,\n",
       " 45,\n",
       " 1084,\n",
       " 945,\n",
       " 81,\n",
       " 1472,\n",
       " 499,\n",
       " 116,\n",
       " 977,\n",
       " 497,\n",
       " 1244,\n",
       " 640,\n",
       " 1760,\n",
       " 3063,\n",
       " 854,\n",
       " 907,\n",
       " 2612,\n",
       " 551,\n",
       " 50,\n",
       " 231,\n",
       " 761,\n",
       " 1118,\n",
       " 53,\n",
       " 37,\n",
       " 461,\n",
       " 4325,\n",
       " 239,\n",
       " 102,\n",
       " 323,\n",
       " 1222,\n",
       " 527,\n",
       " 2430,\n",
       " 745,\n",
       " 266,\n",
       " 328,\n",
       " 3683,\n",
       " 450,\n",
       " 534,\n",
       " 633,\n",
       " 1134,\n",
       " 441,\n",
       " 838,\n",
       " 811,\n",
       " 126,\n",
       " 352,\n",
       " 605,\n",
       " 504,\n",
       " 203,\n",
       " 1481,\n",
       " 200,\n",
       " 50,\n",
       " 1385,\n",
       " 651,\n",
       " 1158,\n",
       " 128,\n",
       " 744,\n",
       " 623,\n",
       " 5746,\n",
       " 105,\n",
       " 57,\n",
       " 786,\n",
       " 624,\n",
       " 26,\n",
       " 655,\n",
       " 617,\n",
       " 593,\n",
       " 909,\n",
       " 56,\n",
       " 871,\n",
       " 314,\n",
       " 110,\n",
       " 35,\n",
       " 1905,\n",
       " 815,\n",
       " 2397,\n",
       " 530,\n",
       " 780,\n",
       " 1177,\n",
       " 951,\n",
       " 1715,\n",
       " 1855,\n",
       " 296,\n",
       " 472,\n",
       " 3915,\n",
       " 530,\n",
       " 36,\n",
       " 1962,\n",
       " 970,\n",
       " 309,\n",
       " 1105,\n",
       " 603,\n",
       " 1063,\n",
       " 180,\n",
       " 819,\n",
       " 40,\n",
       " 44,\n",
       " 426,\n",
       " 61,\n",
       " 27,\n",
       " 1548,\n",
       " 64,\n",
       " 73,\n",
       " 126,\n",
       " 29,\n",
       " 102,\n",
       " 126,\n",
       " 41,\n",
       " 143,\n",
       " 128,\n",
       " 990,\n",
       " 47,\n",
       " 409,\n",
       " 60,\n",
       " 19,\n",
       " 642,\n",
       " 175,\n",
       " 631,\n",
       " 145,\n",
       " 100,\n",
       " 237,\n",
       " 95,\n",
       " 33,\n",
       " 20,\n",
       " 594,\n",
       " 39,\n",
       " 70,\n",
       " 68,\n",
       " 77,\n",
       " 104,\n",
       " 124,\n",
       " 529,\n",
       " 1447,\n",
       " 49,\n",
       " 1136,\n",
       " 51,\n",
       " 213,\n",
       " 98,\n",
       " 37,\n",
       " 1696,\n",
       " 968,\n",
       " 12,\n",
       " 37,\n",
       " 77,\n",
       " 64,\n",
       " 21,\n",
       " 12,\n",
       " 16,\n",
       " 15,\n",
       " 42,\n",
       " 1514,\n",
       " 41,\n",
       " 28,\n",
       " 204,\n",
       " 57,\n",
       " 329,\n",
       " 89,\n",
       " 128,\n",
       " 61,\n",
       " 1343,\n",
       " 2779,\n",
       " 16,\n",
       " 37,\n",
       " 52,\n",
       " 280,\n",
       " 631,\n",
       " 416,\n",
       " 139,\n",
       " 16,\n",
       " 30,\n",
       " 963,\n",
       " 31,\n",
       " 37,\n",
       " 55,\n",
       " 104,\n",
       " 409,\n",
       " 180,\n",
       " 25,\n",
       " 154,\n",
       " 329,\n",
       " 26,\n",
       " 8671,\n",
       " 390,\n",
       " 573,\n",
       " 12,\n",
       " 562,\n",
       " 43,\n",
       " 70,\n",
       " 46,\n",
       " 229,\n",
       " 97,\n",
       " 69,\n",
       " 64,\n",
       " 718,\n",
       " 909,\n",
       " 26,\n",
       " 1067,\n",
       " 2867,\n",
       " 1017,\n",
       " 358,\n",
       " 466,\n",
       " 1616,\n",
       " 94,\n",
       " 243,\n",
       " 56,\n",
       " 1376,\n",
       " 192,\n",
       " 58,\n",
       " 767,\n",
       " 7,\n",
       " 76,\n",
       " 529,\n",
       " 52,\n",
       " 1397,\n",
       " 19,\n",
       " 11,\n",
       " 476,\n",
       " 31,\n",
       " 138,\n",
       " 199,\n",
       " 189,\n",
       " 60,\n",
       " 49,\n",
       " 16,\n",
       " 234,\n",
       " 94,\n",
       " 3013,\n",
       " 82,\n",
       " 211,\n",
       " 181,\n",
       " 66,\n",
       " 27,\n",
       " 49,\n",
       " 306,\n",
       " 148,\n",
       " 854,\n",
       " 1338,\n",
       " 988,\n",
       " 64,\n",
       " 49,\n",
       " 1358,\n",
       " 67,\n",
       " 18,\n",
       " 180,\n",
       " 886,\n",
       " 105,\n",
       " 281,\n",
       " 65,\n",
       " 33,\n",
       " 68,\n",
       " 98,\n",
       " 32,\n",
       " 44,\n",
       " 1022,\n",
       " 135,\n",
       " 145,\n",
       " 61,\n",
       " 363,\n",
       " 115,\n",
       " 99,\n",
       " 314,\n",
       " 146,\n",
       " 50,\n",
       " 90,\n",
       " 908,\n",
       " 30,\n",
       " 59,\n",
       " 111,\n",
       " 270,\n",
       " 499,\n",
       " 875,\n",
       " 6,\n",
       " 220,\n",
       " 110,\n",
       " 470,\n",
       " 86,\n",
       " 19,\n",
       " 270,\n",
       " 74,\n",
       " 302,\n",
       " 18,\n",
       " 24,\n",
       " 331,\n",
       " 25,\n",
       " 162,\n",
       " 9,\n",
       " 209,\n",
       " 4669,\n",
       " 37,\n",
       " 34,\n",
       " 34,\n",
       " 109,\n",
       " 136,\n",
       " 147,\n",
       " 942,\n",
       " 128,\n",
       " 55,\n",
       " 90,\n",
       " 17,\n",
       " 71,\n",
       " 100,\n",
       " 216,\n",
       " 132,\n",
       " 188,\n",
       " 453,\n",
       " 82,\n",
       " 134,\n",
       " 38,\n",
       " 1105,\n",
       " 41,\n",
       " 98,\n",
       " 15,\n",
       " 57,\n",
       " 14,\n",
       " 998,\n",
       " 362,\n",
       " 17,\n",
       " 1625,\n",
       " 37,\n",
       " 179,\n",
       " 151,\n",
       " 43,\n",
       " 47,\n",
       " 45,\n",
       " 74,\n",
       " 35,\n",
       " 180,\n",
       " 175,\n",
       " 47,\n",
       " 81,\n",
       " 510,\n",
       " 207,\n",
       " 65,\n",
       " 75,\n",
       " 67,\n",
       " 28,\n",
       " 111,\n",
       " 1069,\n",
       " 69,\n",
       " 939,\n",
       " 60,\n",
       " 72,\n",
       " 223,\n",
       " 2197,\n",
       " 259,\n",
       " 263,\n",
       " 35,\n",
       " 116,\n",
       " 712,\n",
       " 70,\n",
       " 9,\n",
       " 595,\n",
       " 24,\n",
       " 50,\n",
       " 2800,\n",
       " 898,\n",
       " 82,\n",
       " 122,\n",
       " 48,\n",
       " 225,\n",
       " 191,\n",
       " 25,\n",
       " 96,\n",
       " 34,\n",
       " 32,\n",
       " 85,\n",
       " 6,\n",
       " 54,\n",
       " 605,\n",
       " 9,\n",
       " 236,\n",
       " 115,\n",
       " 386,\n",
       " 36,\n",
       " 453,\n",
       " 896,\n",
       " 14,\n",
       " 18,\n",
       " 44,\n",
       " 1294,\n",
       " 188,\n",
       " 96,\n",
       " 193,\n",
       " 145,\n",
       " 620,\n",
       " 81,\n",
       " 33,\n",
       " 44,\n",
       " 60,\n",
       " 697,\n",
       " 194,\n",
       " 63,\n",
       " 7224,\n",
       " 152,\n",
       " 197,\n",
       " 577,\n",
       " 51,\n",
       " 30,\n",
       " 365,\n",
       " 30,\n",
       " 396,\n",
       " 260,\n",
       " 13,\n",
       " 57,\n",
       " 840,\n",
       " 43,\n",
       " 76,\n",
       " 1120,\n",
       " 728,\n",
       " 618,\n",
       " 58,\n",
       " 867,\n",
       " 1448,\n",
       " 303,\n",
       " 148,\n",
       " 20,\n",
       " 205,\n",
       " 246,\n",
       " 241,\n",
       " 30,\n",
       " 1153,\n",
       " 21,\n",
       " 2314,\n",
       " 55,\n",
       " 148,\n",
       " 177,\n",
       " 65,\n",
       " 78,\n",
       " 47,\n",
       " 53,\n",
       " 48,\n",
       " 129,\n",
       " 569,\n",
       " 123,\n",
       " 98,\n",
       " 426,\n",
       " 47,\n",
       " 46,\n",
       " 57,\n",
       " 60,\n",
       " 286,\n",
       " 87,\n",
       " 2068,\n",
       " 58,\n",
       " 152,\n",
       " 832,\n",
       " 43,\n",
       " 85,\n",
       " 469,\n",
       " 176,\n",
       " 53,\n",
       " 709,\n",
       " 687,\n",
       " 955,\n",
       " 808,\n",
       " 61,\n",
       " 492,\n",
       " 1161,\n",
       " 57,\n",
       " 264,\n",
       " 75,\n",
       " 2630,\n",
       " 103,\n",
       " 96,\n",
       " 477,\n",
       " 408,\n",
       " 1185,\n",
       " 512,\n",
       " 290,\n",
       " 148,\n",
       " 83,\n",
       " 2262,\n",
       " 47,\n",
       " 361,\n",
       " 63,\n",
       " 52,\n",
       " 60,\n",
       " 65,\n",
       " 104,\n",
       " 1478,\n",
       " 74,\n",
       " 312,\n",
       " 52,\n",
       " 59,\n",
       " 44,\n",
       " 2113,\n",
       " 107,\n",
       " 6323,\n",
       " 43,\n",
       " 751,\n",
       " 56,\n",
       " 46,\n",
       " 58,\n",
       " 250,\n",
       " 471,\n",
       " 65,\n",
       " 55,\n",
       " 47,\n",
       " 52,\n",
       " 75,\n",
       " 2491,\n",
       " 83,\n",
       " 92,\n",
       " 111,\n",
       " 1563,\n",
       " 744,\n",
       " 68,\n",
       " 807,\n",
       " 94,\n",
       " 108,\n",
       " 69,\n",
       " 1093,\n",
       " 647,\n",
       " 65,\n",
       " 974,\n",
       " 502,\n",
       " 1022,\n",
       " 1246,\n",
       " 2432,\n",
       " 182,\n",
       " 855,\n",
       " 173,\n",
       " 751,\n",
       " 3350,\n",
       " 29,\n",
       " 3171,\n",
       " 331,\n",
       " 81,\n",
       " 193,\n",
       " 217,\n",
       " 421,\n",
       " 632,\n",
       " 2530,\n",
       " 680,\n",
       " 59,\n",
       " 1609,\n",
       " 3199,\n",
       " 37,\n",
       " 50,\n",
       " 47,\n",
       " 47,\n",
       " 296,\n",
       " 9,\n",
       " 51,\n",
       " 49,\n",
       " 53,\n",
       " 45,\n",
       " 85,\n",
       " 917,\n",
       " 52,\n",
       " 47,\n",
       " 1989,\n",
       " 859,\n",
       " 37,\n",
       " 98,\n",
       " 826,\n",
       " 2368,\n",
       " 52,\n",
       " 671,\n",
       " 1949,\n",
       " 502,\n",
       " 157,\n",
       " 277,\n",
       " 45,\n",
       " 681,\n",
       " 50,\n",
       " 73,\n",
       " 1675,\n",
       " 329,\n",
       " 63,\n",
       " 110,\n",
       " 39,\n",
       " 337,\n",
       " 86,\n",
       " 18,\n",
       " 3824,\n",
       " 48,\n",
       " 74,\n",
       " 151,\n",
       " 198,\n",
       " 910,\n",
       " 51,\n",
       " 58,\n",
       " 70,\n",
       " 773,\n",
       " 43,\n",
       " 62,\n",
       " 40,\n",
       " 17,\n",
       " 1090,\n",
       " 107,\n",
       " 42,\n",
       " 64,\n",
       " 82,\n",
       " 117,\n",
       " 104,\n",
       " 119,\n",
       " 77,\n",
       " 798,\n",
       " 105,\n",
       " 208,\n",
       " 2705,\n",
       " 79,\n",
       " 1017,\n",
       " 40,\n",
       " 830,\n",
       " 451,\n",
       " 2411,\n",
       " 790,\n",
       " 266,\n",
       " 140,\n",
       " 292,\n",
       " 66,\n",
       " 79,\n",
       " 49,\n",
       " 41,\n",
       " 742,\n",
       " 1055,\n",
       " 784,\n",
       " 71,\n",
       " 286,\n",
       " 90,\n",
       " 1088,\n",
       " 1024,\n",
       " 30,\n",
       " 1255,\n",
       " 553,\n",
       " 62,\n",
       " 1614,\n",
       " 313,\n",
       " 1032,\n",
       " 159,\n",
       " 1075,\n",
       " 921,\n",
       " 61,\n",
       " 2141,\n",
       " 60,\n",
       " 37,\n",
       " 1251,\n",
       " 61,\n",
       " 340,\n",
       " 3886,\n",
       " 565,\n",
       " 809,\n",
       " 92,\n",
       " 74,\n",
       " 64,\n",
       " 2099,\n",
       " 3885,\n",
       " 219,\n",
       " 55,\n",
       " 668,\n",
       " 149,\n",
       " 3390,\n",
       " 236,\n",
       " 225,\n",
       " 42,\n",
       " 501,\n",
       " 52,\n",
       " 1459,\n",
       " 967,\n",
       " 55,\n",
       " 290,\n",
       " 1370,\n",
       " 138,\n",
       " 301,\n",
       " 184,\n",
       " 773,\n",
       " 2388,\n",
       " 5537,\n",
       " 36,\n",
       " 62,\n",
       " 3117,\n",
       " 623,\n",
       " 344,\n",
       " 103,\n",
       " 68,\n",
       " 248,\n",
       " 3714,\n",
       " 55,\n",
       " 936,\n",
       " 754,\n",
       " 1544,\n",
       " 82,\n",
       " 134,\n",
       " 483,\n",
       " 1177,\n",
       " 73,\n",
       " 52,\n",
       " 50,\n",
       " 56,\n",
       " 193,\n",
       " 405,\n",
       " 39,\n",
       " 586,\n",
       " 522,\n",
       " 53,\n",
       " 94,\n",
       " 127,\n",
       " 1195,\n",
       " 1256,\n",
       " 185,\n",
       " 219,\n",
       " 70,\n",
       " 68,\n",
       " 51,\n",
       " 29,\n",
       " 107,\n",
       " 344,\n",
       " 263,\n",
       " 1149,\n",
       " 39,\n",
       " 49,\n",
       " 84,\n",
       " 1652,\n",
       " 47,\n",
       " 42,\n",
       " 64,\n",
       " 874,\n",
       " 126,\n",
       " 208,\n",
       " 43,\n",
       " 56,\n",
       " 40,\n",
       " 66,\n",
       " 102,\n",
       " 280,\n",
       " 155,\n",
       " 156,\n",
       " 101,\n",
       " 51,\n",
       " 132,\n",
       " 1082,\n",
       " 389,\n",
       " 75,\n",
       " 60,\n",
       " 357,\n",
       " 356,\n",
       " 66,\n",
       " 89,\n",
       " 2285,\n",
       " 139,\n",
       " 49,\n",
       " 1510,\n",
       " 84,\n",
       " 46,\n",
       " 57,\n",
       " 61,\n",
       " 347,\n",
       " 29,\n",
       " 55,\n",
       " 564,\n",
       " 1431,\n",
       " 56,\n",
       " 88,\n",
       " 479,\n",
       " 656,\n",
       " 92,\n",
       " 245,\n",
       " 44,\n",
       " 114,\n",
       " 57,\n",
       " 84,\n",
       " 579,\n",
       " 51,\n",
       " 950,\n",
       " 45,\n",
       " 842,\n",
       " 353,\n",
       " 659,\n",
       " 101,\n",
       " 2556,\n",
       " 44,\n",
       " 42,\n",
       " 50,\n",
       " 313,\n",
       " 85,\n",
       " 117,\n",
       " 232,\n",
       " 275,\n",
       " 509,\n",
       " 99,\n",
       " 250,\n",
       " 1698,\n",
       " 87,\n",
       " 54,\n",
       " 2823,\n",
       " 75,\n",
       " 34,\n",
       " 109,\n",
       " 73,\n",
       " 53,\n",
       " 97,\n",
       " 48,\n",
       " 34,\n",
       " 34,\n",
       " 76,\n",
       " 49,\n",
       " 53,\n",
       " 934,\n",
       " 458,\n",
       " 1333,\n",
       " 106,\n",
       " 625,\n",
       " 335,\n",
       " 1496,\n",
       " 358,\n",
       " 195,\n",
       " 952,\n",
       " 55,\n",
       " 145,\n",
       " 739,\n",
       " 75,\n",
       " 495,\n",
       " 80,\n",
       " 230,\n",
       " 35,\n",
       " 1020,\n",
       " 174,\n",
       " 55,\n",
       " 614,\n",
       " 985,\n",
       " 56,\n",
       " 125,\n",
       " 44,\n",
       " 204,\n",
       " 804,\n",
       " 187,\n",
       " 77,\n",
       " 672,\n",
       " 294,\n",
       " 74,\n",
       " 62,\n",
       " 707,\n",
       " 74,\n",
       " 100,\n",
       " 59,\n",
       " 1673,\n",
       " 60,\n",
       " 67,\n",
       " 51,\n",
       " 54,\n",
       " 57,\n",
       " 544,\n",
       " 1012,\n",
       " 1215,\n",
       " 486,\n",
       " 232,\n",
       " 485,\n",
       " 76,\n",
       " 54,\n",
       " 50,\n",
       " 241,\n",
       " 429,\n",
       " 56,\n",
       " 118,\n",
       " 72,\n",
       " 126,\n",
       " 64,\n",
       " 55,\n",
       " 52,\n",
       " 113,\n",
       " 53,\n",
       " 52,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=return_dataloader(X_train,y_0,params,is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_labels = batch[1]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids[0][:20,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 13:27:07.329630 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "I0924 13:27:08.513124 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): cdn.huggingface.co\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_path': 'bert-base-multilingual-cased', 'max_length': 128, 'batch_size': 6, 'weights': [1.0, 1.0], 'data_path': '../Data/New_Data_15-06-2020/Fearspeech_data_final.pkl', 'max_sentences_per_doc': 10, 'transformer_type': 'lstm_transformer', 'device': 'cuda', 'learning_rate': 2e-05, 'epsilon': 1e-08, 'random_seed': 2, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing DocumentBERTLSTM: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DocumentBERTLSTM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DocumentBERTLSTM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DocumentBERTLSTM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'classifier.1.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=select_transformer_model(params['transformer_type'],params['model_path'],params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (2): Tanh()\n",
       ")>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
