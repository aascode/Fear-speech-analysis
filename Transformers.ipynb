{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:28.091363Z",
     "start_time": "2020-09-24T19:12:28.082450Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:29.372530Z",
     "start_time": "2020-09-24T19:12:28.238380Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from models.tokenization import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from models.model_architecture import *\n",
    "from models.train_eval import *\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "parent_path='../Data/New_Data_15-06-2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:02:17.787654Z",
     "start_time": "2020-09-24T19:02:17.483899Z"
    }
   },
   "outputs": [],
   "source": [
    "annotated_df=pd.read_pickle(parent_path+'Fearspeech_data_final.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:02:18.150630Z",
     "start_time": "2020-09-24T19:02:18.097236Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:02:18.796314Z",
     "start_time": "2020-09-24T19:02:18.777850Z"
    }
   },
   "outputs": [],
   "source": [
    "params={'model_path':'xlm-roberta-base',\n",
    "        'max_length':128,\n",
    "        'batch_size':6,\n",
    "        'weights':[1.0,1.0],\n",
    "        'data_path':parent_path+'Fearspeech_data_final.pkl',\n",
    "        'max_sentences_per_doc':10,\n",
    "        'transformer_type':'lstm_transformer',\n",
    "        'device':'cuda',\n",
    "        'learning_rate':2e-5,\n",
    "        'epsilon':1e-8,\n",
    "        'random_seed':2,\n",
    "        'epochs':5\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:03:06.862173Z",
     "start_time": "2020-09-24T19:02:19.471487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [00:36<00:00, 216.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c9e65209bca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Punyajoy_folders/works_2020/Fear-speech-analysis/models/train_eval.py\u001b[0m in \u001b[0;36mtrain_phase\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading tokenizer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"bert-base-japanese\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             )\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "train_phase(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 09:30:24.217592 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "I0924 09:30:25.342872 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=tokenizer.tokenize(\"I am goof\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=['<s>']+temp+['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:46.241374Z",
     "start_time": "2020-09-24T19:12:36.073855Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f47bba9c8e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlm-roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/punyajoy_gpu/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             )\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'xlm-roberta-base'. Make sure that:\n\n- 'xlm-roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'xlm-roberta-base' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config=AutoConfig.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T19:12:46.242703Z",
     "start_time": "2020-09-24T19:23:51.062Z"
    }
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 09:25:16.440173 140120072415040 word.py:92] Falling back to default tokenizer, the NLTK's `TreebankWordTokenizer()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d80c4c2cf0c4f52a7a483be31930723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "annotated_df=pd.read_pickle(params['data_path'])\n",
    "params_preprocess={'remove_numbers': True, 'remove_emoji': True, 'remove_stop_words': False, 'tokenize': False}\n",
    "list_sents=[preprocess_sent(ele,params=params_preprocess).strip() for ele in tqdm_notebook(annotated_df['message_text'],total=len(annotated_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ऐसी विचारधारा मानवता के लिए खतरा है ऐसी विचारधारा का समाप्त होना अति आवश्यक है  भाई अब्दुस समीर शेख की पोस्ट जो साभार कापी पेस्ट की गयी है    श्रीलंका के करुनेगला में एक काफी पुराना और प्रतिष्ठित अस्पताल है करुनेगला टेक्निकल हॉस्पिटल  पूरे शहर में एक ही अस्पताल है तो काफी भीड़ रहती है चूंकि अस्पताल प्रतिष्ठित है स्पेशलिस्ट डाक्टर भी खूब हैं तो दूरदूर से मरीज़ आते हैं  इसी अस्पताल के स्त्री रोग विभाग के अध्यक्ष डाक्टर मोहम्मद सीगु सियाब्दीन हैं जो गाइनकॉलजिस्ट हैं  वक्त के नमाज़ी मुसलमान हैं             डॉक्टर मोहम्मद सियाब्दीन मुस्लिम स्त्रियों का नसबंदी ऑपरेशन करने के बजाय उन्हें नसबंदी के नुकसान गिनाते थे  डाक्टर साहब के पास कोलंबों और करुनेगला जैसी जगहों पर  प्रॉपर्टी थींजिसकी कीमत रु  करोड़ हैं  उन्ही के अस्पताल में एक सिंहली गर्भवती नर्स को प्रसव का दर्द हुआ तो डॉक्टर मोहम्मद सियाब्दीन के पास पहुची डाक्टर ने ऑपरेशन की सलाह दी नर्स को तुरन्त एनस्थीसिया दिया गया ऑपरेशन किया गया जब होश आया तो पता चला कि भ्रूण जीवित नहीं पाया गया कुछ दिन बाद नर्स रिलीव होकर घर आयी तो उसे अपने शरीर मे कुछ अजीब समस्याओं से दोचार होना पड़ा  अल्ट्रासाउंड हुआ तो पता चला कि नर्स का यूटेरस बच्चेदानी निकाली जा चुकी है  डाक्टर मोहम्मद सियाब्दीन से पुलिस ने पूंछतांछ की तो पता चला कि बच्चेदानीडाक्टर सियाब्दीन ने बगैर किसी को संज्ञान में लिए चुपके से निकालकर भ्रूण को नष्ट कर दिया है  जब खबर अखबार में छपी और चैनलों में चली तो श्रीलंका में तूफान आ गयाकड़ी पूंछतांछ में डाक्टर सियाब्दीन ने बताया कि पिछले  सालों में उसने ऐसे लगभग  ऑपरेशन किये हैंजिसमे नसबंदी करके सिंहली और हिन्दू औरतों को बांझ बनाया गया था  काफी डाटा डाक्टर के कम्प्यूटर और लैपटॉप से भी बरामद हुआ है  अब चीजें खुल कर आ रही हैं डाक्टर मोहम्मद सीगु सियाब्दीन साफी आतंकी संगठन तौहीद जमात से जुड़ा है यह इस्लामिक संगठनisis से सीधे जुड़ा है और श्रीलंका में अभी चर्चो और होटल में हुए बम विस्फोटों में  लोगों की हत्या के लिए ज़िम्मेदार है  पिछले  वर्षों में हज़ारों सिंहली और हिन्दू औरतों को नसबंदी और यूटेरस निकाल कर बांझ बनानाआज तक का सबसे घिनौना अपराध इसलिए भी है क्योंकि डाक्टर मोहम्मद सियाब्दीन ने उन हज़ारों बच्चों की हत्या कर दीजो पैदा होने वाले थे परिवारों की पीढ़ियां की पीढ़ियां खत्म कर दीं इस डाक्टर ने  जिसका प्रभाव अनंतकाल तक चलता रहेगा   यह जिहाद अमेरिका में wtc पर हमला कर  लोगों को मारने से भी बड़ा है  फ्रांस में एक जेहादी द्वारा ट्रक से कुचल कर  लोगों को मार देने जैसा अपराध तो इसके सामने कुछ भी नहीं    इस अपराध के आगे सारे अपराध बौने हो गये  क्योंकि मरीज़ डाक्टर को भगवान मानता है वही डाक्टर उसकी संतानों और आगे आने वाली पीढ़ियों की प्रत्याशा खत्म कर दे सिर्फ इसलिए कि उनका धर्म इस्लाम कहता है काफिर को जीने का हक नहीं इसलिए भ्रूण बनने की संभावना को नसबंदी कर समाप्त कर दिया जाये  क्या ऐसा जघन्य अपराध कोई और धर्मावलंबी व्यक्ति कर सकता है नहीं ऐसा जघन्य अपराध सिर्फ मुसलमान ही कर सकता है',\n",
       " 'aimim president hyderabad mp barrister janab asaduddin owaisi sahab in an interview with indian express said \"muslims have borne a disproportionate burden in ‘saving secularism now it is time that one demands justice and a fairer share as long we vote with fear and not hope  our lot is unlikely to improve\"',\n",
       " 'जिनको जिहाद कर के मासूम लोगों को मार के जन्नत और  हूर मिलती हो उन जैसे मूर्खों को ही महागठबंधन में भारत का भविष्य दिखता हैं',\n",
       " 'सनातन धर्म की जय हो     सनातन धर्म रक्षक समिति      कश्मीर का पहला आतंकी मुक्त जिला बना बारामूलासेना और कश्मीर पुलिस ने आतंकवादियों को ठोकते हुए पायी कठिन सफलता   जम्मूकश्मीर के बारामूला जिले को हिज्बुल का गढ़ कहा जाता था लेकिन सेना और पुलिस ने इसे आतंक के दंश के आजाद करा लिया है बुधवार को बारामूला की मुठभेड़ में तीन आतंकियों के मारे जाने के बाद पुलिस ने इसे  आतंक मुक्त जिला घोषित किया है  बारामूला कश्मीर का ऐसा आतंक प्रभाविक जिलों में ऐसा पहला जिला बन गया है जहां कोई जिंदा आतंकी नहीं बचा है  जम्मूकश्मीर पुलिस ने इस कामयाबी के लिए स्थानीय जनता को शुक्रिया अदा किया कि उन्होंने जिले में सुरक्षा के लिए लिहाज से बेहतर माहौल दिया  जम्मूकश्मीर पुलिस के डीजीपी दिलबाग सिंह ने इसकी पुष्टि करते हुए कहा बारामूला जिले में बुधवार के ऑपरेशन में  आतंकी मार गिराए गए जिसके बाद ये पहला आतंकी मुक्त जिला बन गया है  सनातन धर्मरक्षक समिति से जुड़ने हेतु तथा अपनी सेवा देने हेतु नीचे दी गई लिंक पर जाकर संपर्क करें      जनजागृति हेतु संदेश को  पढ़ने के उपरांत अवश्य साझा करें  जय श्रीराम  वन्देमातरम्',\n",
       " 'उज्जैन का एक बूथ नंबर  है जिसमें  मुस्लिम वोट है पहली बात तो वोटिंग की कर लेते है यदि ये वोट हिन्दू होते तो  के आस पास ही वोट डालते अब रही बात विकास की राष्ट्रहित की सब रह गया ताक में धरा का धरा इन्होंने  के पूरे   वोट डाले है और बीजेपी को मिले है  वोट  वोट इन्होंने आतंकवाद और जिहाद के लिया कांग्रेस को दिया है अब आप सबूत की बात करेंगे वो सबूत भी है']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.sample(list_sents,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=[]\n",
    "for sent in list_sents:\n",
    "    count.append(len(tokenizer.tokenize(sent)))\n",
    "\n",
    "#X_0 = np.array(list_sents,dtype='object')\n",
    "# y_0 = np.array(annotated_df['one_fear_speech'])\n",
    "# tokenizer = AutoTokenizer.from_pretrained(params['model_path'])\n",
    "    \n",
    "# X_train, X_train_length = encode_documents(X_0,params,tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_greater=0\n",
    "for ele in count:\n",
    "    if(ele > (128*5)):\n",
    "        count_greater+=1\n",
    "count_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 85,\n",
       " 392,\n",
       " 42,\n",
       " 1154,\n",
       " 1245,\n",
       " 2141,\n",
       " 153,\n",
       " 340,\n",
       " 985,\n",
       " 246,\n",
       " 30,\n",
       " 209,\n",
       " 570,\n",
       " 542,\n",
       " 1067,\n",
       " 837,\n",
       " 594,\n",
       " 869,\n",
       " 1014,\n",
       " 895,\n",
       " 49,\n",
       " 667,\n",
       " 1035,\n",
       " 3217,\n",
       " 45,\n",
       " 86,\n",
       " 666,\n",
       " 1395,\n",
       " 311,\n",
       " 33,\n",
       " 137,\n",
       " 627,\n",
       " 93,\n",
       " 52,\n",
       " 210,\n",
       " 5030,\n",
       " 539,\n",
       " 1452,\n",
       " 3111,\n",
       " 588,\n",
       " 1052,\n",
       " 86,\n",
       " 65,\n",
       " 44,\n",
       " 478,\n",
       " 43,\n",
       " 1478,\n",
       " 491,\n",
       " 1261,\n",
       " 269,\n",
       " 2313,\n",
       " 308,\n",
       " 1163,\n",
       " 1262,\n",
       " 402,\n",
       " 1441,\n",
       " 322,\n",
       " 883,\n",
       " 143,\n",
       " 265,\n",
       " 415,\n",
       " 494,\n",
       " 459,\n",
       " 1012,\n",
       " 608,\n",
       " 1423,\n",
       " 1275,\n",
       " 201,\n",
       " 534,\n",
       " 890,\n",
       " 784,\n",
       " 438,\n",
       " 389,\n",
       " 887,\n",
       " 634,\n",
       " 1419,\n",
       " 123,\n",
       " 311,\n",
       " 135,\n",
       " 1622,\n",
       " 432,\n",
       " 776,\n",
       " 1189,\n",
       " 805,\n",
       " 876,\n",
       " 819,\n",
       " 250,\n",
       " 480,\n",
       " 428,\n",
       " 946,\n",
       " 832,\n",
       " 884,\n",
       " 85,\n",
       " 197,\n",
       " 44,\n",
       " 138,\n",
       " 1726,\n",
       " 646,\n",
       " 1715,\n",
       " 323,\n",
       " 3072,\n",
       " 3307,\n",
       " 202,\n",
       " 60,\n",
       " 645,\n",
       " 1178,\n",
       " 229,\n",
       " 858,\n",
       " 584,\n",
       " 2242,\n",
       " 2228,\n",
       " 53,\n",
       " 640,\n",
       " 1551,\n",
       " 264,\n",
       " 596,\n",
       " 221,\n",
       " 2835,\n",
       " 975,\n",
       " 940,\n",
       " 200,\n",
       " 351,\n",
       " 788,\n",
       " 47,\n",
       " 2251,\n",
       " 74,\n",
       " 450,\n",
       " 1633,\n",
       " 396,\n",
       " 89,\n",
       " 789,\n",
       " 53,\n",
       " 178,\n",
       " 667,\n",
       " 962,\n",
       " 42,\n",
       " 30,\n",
       " 45,\n",
       " 1084,\n",
       " 945,\n",
       " 81,\n",
       " 1472,\n",
       " 499,\n",
       " 116,\n",
       " 977,\n",
       " 497,\n",
       " 1244,\n",
       " 640,\n",
       " 1760,\n",
       " 3063,\n",
       " 854,\n",
       " 907,\n",
       " 2612,\n",
       " 551,\n",
       " 50,\n",
       " 231,\n",
       " 761,\n",
       " 1118,\n",
       " 53,\n",
       " 37,\n",
       " 461,\n",
       " 4325,\n",
       " 239,\n",
       " 102,\n",
       " 323,\n",
       " 1222,\n",
       " 527,\n",
       " 2430,\n",
       " 745,\n",
       " 266,\n",
       " 328,\n",
       " 3683,\n",
       " 450,\n",
       " 534,\n",
       " 633,\n",
       " 1134,\n",
       " 441,\n",
       " 838,\n",
       " 811,\n",
       " 126,\n",
       " 352,\n",
       " 605,\n",
       " 504,\n",
       " 203,\n",
       " 1481,\n",
       " 200,\n",
       " 50,\n",
       " 1385,\n",
       " 651,\n",
       " 1158,\n",
       " 128,\n",
       " 744,\n",
       " 623,\n",
       " 5746,\n",
       " 105,\n",
       " 57,\n",
       " 786,\n",
       " 624,\n",
       " 26,\n",
       " 655,\n",
       " 617,\n",
       " 593,\n",
       " 909,\n",
       " 56,\n",
       " 871,\n",
       " 314,\n",
       " 110,\n",
       " 35,\n",
       " 1905,\n",
       " 815,\n",
       " 2397,\n",
       " 530,\n",
       " 780,\n",
       " 1177,\n",
       " 951,\n",
       " 1715,\n",
       " 1855,\n",
       " 296,\n",
       " 472,\n",
       " 3915,\n",
       " 530,\n",
       " 36,\n",
       " 1962,\n",
       " 970,\n",
       " 309,\n",
       " 1105,\n",
       " 603,\n",
       " 1063,\n",
       " 180,\n",
       " 819,\n",
       " 40,\n",
       " 44,\n",
       " 426,\n",
       " 61,\n",
       " 27,\n",
       " 1548,\n",
       " 64,\n",
       " 73,\n",
       " 126,\n",
       " 29,\n",
       " 102,\n",
       " 126,\n",
       " 41,\n",
       " 143,\n",
       " 128,\n",
       " 990,\n",
       " 47,\n",
       " 409,\n",
       " 60,\n",
       " 19,\n",
       " 642,\n",
       " 175,\n",
       " 631,\n",
       " 145,\n",
       " 100,\n",
       " 237,\n",
       " 95,\n",
       " 33,\n",
       " 20,\n",
       " 594,\n",
       " 39,\n",
       " 70,\n",
       " 68,\n",
       " 77,\n",
       " 104,\n",
       " 124,\n",
       " 529,\n",
       " 1447,\n",
       " 49,\n",
       " 1136,\n",
       " 51,\n",
       " 213,\n",
       " 98,\n",
       " 37,\n",
       " 1696,\n",
       " 968,\n",
       " 12,\n",
       " 37,\n",
       " 77,\n",
       " 64,\n",
       " 21,\n",
       " 12,\n",
       " 16,\n",
       " 15,\n",
       " 42,\n",
       " 1514,\n",
       " 41,\n",
       " 28,\n",
       " 204,\n",
       " 57,\n",
       " 329,\n",
       " 89,\n",
       " 128,\n",
       " 61,\n",
       " 1343,\n",
       " 2779,\n",
       " 16,\n",
       " 37,\n",
       " 52,\n",
       " 280,\n",
       " 631,\n",
       " 416,\n",
       " 139,\n",
       " 16,\n",
       " 30,\n",
       " 963,\n",
       " 31,\n",
       " 37,\n",
       " 55,\n",
       " 104,\n",
       " 409,\n",
       " 180,\n",
       " 25,\n",
       " 154,\n",
       " 329,\n",
       " 26,\n",
       " 8671,\n",
       " 390,\n",
       " 573,\n",
       " 12,\n",
       " 562,\n",
       " 43,\n",
       " 70,\n",
       " 46,\n",
       " 229,\n",
       " 97,\n",
       " 69,\n",
       " 64,\n",
       " 718,\n",
       " 909,\n",
       " 26,\n",
       " 1067,\n",
       " 2867,\n",
       " 1017,\n",
       " 358,\n",
       " 466,\n",
       " 1616,\n",
       " 94,\n",
       " 243,\n",
       " 56,\n",
       " 1376,\n",
       " 192,\n",
       " 58,\n",
       " 767,\n",
       " 7,\n",
       " 76,\n",
       " 529,\n",
       " 52,\n",
       " 1397,\n",
       " 19,\n",
       " 11,\n",
       " 476,\n",
       " 31,\n",
       " 138,\n",
       " 199,\n",
       " 189,\n",
       " 60,\n",
       " 49,\n",
       " 16,\n",
       " 234,\n",
       " 94,\n",
       " 3013,\n",
       " 82,\n",
       " 211,\n",
       " 181,\n",
       " 66,\n",
       " 27,\n",
       " 49,\n",
       " 306,\n",
       " 148,\n",
       " 854,\n",
       " 1338,\n",
       " 988,\n",
       " 64,\n",
       " 49,\n",
       " 1358,\n",
       " 67,\n",
       " 18,\n",
       " 180,\n",
       " 886,\n",
       " 105,\n",
       " 281,\n",
       " 65,\n",
       " 33,\n",
       " 68,\n",
       " 98,\n",
       " 32,\n",
       " 44,\n",
       " 1022,\n",
       " 135,\n",
       " 145,\n",
       " 61,\n",
       " 363,\n",
       " 115,\n",
       " 99,\n",
       " 314,\n",
       " 146,\n",
       " 50,\n",
       " 90,\n",
       " 908,\n",
       " 30,\n",
       " 59,\n",
       " 111,\n",
       " 270,\n",
       " 499,\n",
       " 875,\n",
       " 6,\n",
       " 220,\n",
       " 110,\n",
       " 470,\n",
       " 86,\n",
       " 19,\n",
       " 270,\n",
       " 74,\n",
       " 302,\n",
       " 18,\n",
       " 24,\n",
       " 331,\n",
       " 25,\n",
       " 162,\n",
       " 9,\n",
       " 209,\n",
       " 4669,\n",
       " 37,\n",
       " 34,\n",
       " 34,\n",
       " 109,\n",
       " 136,\n",
       " 147,\n",
       " 942,\n",
       " 128,\n",
       " 55,\n",
       " 90,\n",
       " 17,\n",
       " 71,\n",
       " 100,\n",
       " 216,\n",
       " 132,\n",
       " 188,\n",
       " 453,\n",
       " 82,\n",
       " 134,\n",
       " 38,\n",
       " 1105,\n",
       " 41,\n",
       " 98,\n",
       " 15,\n",
       " 57,\n",
       " 14,\n",
       " 998,\n",
       " 362,\n",
       " 17,\n",
       " 1625,\n",
       " 37,\n",
       " 179,\n",
       " 151,\n",
       " 43,\n",
       " 47,\n",
       " 45,\n",
       " 74,\n",
       " 35,\n",
       " 180,\n",
       " 175,\n",
       " 47,\n",
       " 81,\n",
       " 510,\n",
       " 207,\n",
       " 65,\n",
       " 75,\n",
       " 67,\n",
       " 28,\n",
       " 111,\n",
       " 1069,\n",
       " 69,\n",
       " 939,\n",
       " 60,\n",
       " 72,\n",
       " 223,\n",
       " 2197,\n",
       " 259,\n",
       " 263,\n",
       " 35,\n",
       " 116,\n",
       " 712,\n",
       " 70,\n",
       " 9,\n",
       " 595,\n",
       " 24,\n",
       " 50,\n",
       " 2800,\n",
       " 898,\n",
       " 82,\n",
       " 122,\n",
       " 48,\n",
       " 225,\n",
       " 191,\n",
       " 25,\n",
       " 96,\n",
       " 34,\n",
       " 32,\n",
       " 85,\n",
       " 6,\n",
       " 54,\n",
       " 605,\n",
       " 9,\n",
       " 236,\n",
       " 115,\n",
       " 386,\n",
       " 36,\n",
       " 453,\n",
       " 896,\n",
       " 14,\n",
       " 18,\n",
       " 44,\n",
       " 1294,\n",
       " 188,\n",
       " 96,\n",
       " 193,\n",
       " 145,\n",
       " 620,\n",
       " 81,\n",
       " 33,\n",
       " 44,\n",
       " 60,\n",
       " 697,\n",
       " 194,\n",
       " 63,\n",
       " 7224,\n",
       " 152,\n",
       " 197,\n",
       " 577,\n",
       " 51,\n",
       " 30,\n",
       " 365,\n",
       " 30,\n",
       " 396,\n",
       " 260,\n",
       " 13,\n",
       " 57,\n",
       " 840,\n",
       " 43,\n",
       " 76,\n",
       " 1120,\n",
       " 728,\n",
       " 618,\n",
       " 58,\n",
       " 867,\n",
       " 1448,\n",
       " 303,\n",
       " 148,\n",
       " 20,\n",
       " 205,\n",
       " 246,\n",
       " 241,\n",
       " 30,\n",
       " 1153,\n",
       " 21,\n",
       " 2314,\n",
       " 55,\n",
       " 148,\n",
       " 177,\n",
       " 65,\n",
       " 78,\n",
       " 47,\n",
       " 53,\n",
       " 48,\n",
       " 129,\n",
       " 569,\n",
       " 123,\n",
       " 98,\n",
       " 426,\n",
       " 47,\n",
       " 46,\n",
       " 57,\n",
       " 60,\n",
       " 286,\n",
       " 87,\n",
       " 2068,\n",
       " 58,\n",
       " 152,\n",
       " 832,\n",
       " 43,\n",
       " 85,\n",
       " 469,\n",
       " 176,\n",
       " 53,\n",
       " 709,\n",
       " 687,\n",
       " 955,\n",
       " 808,\n",
       " 61,\n",
       " 492,\n",
       " 1161,\n",
       " 57,\n",
       " 264,\n",
       " 75,\n",
       " 2630,\n",
       " 103,\n",
       " 96,\n",
       " 477,\n",
       " 408,\n",
       " 1185,\n",
       " 512,\n",
       " 290,\n",
       " 148,\n",
       " 83,\n",
       " 2262,\n",
       " 47,\n",
       " 361,\n",
       " 63,\n",
       " 52,\n",
       " 60,\n",
       " 65,\n",
       " 104,\n",
       " 1478,\n",
       " 74,\n",
       " 312,\n",
       " 52,\n",
       " 59,\n",
       " 44,\n",
       " 2113,\n",
       " 107,\n",
       " 6323,\n",
       " 43,\n",
       " 751,\n",
       " 56,\n",
       " 46,\n",
       " 58,\n",
       " 250,\n",
       " 471,\n",
       " 65,\n",
       " 55,\n",
       " 47,\n",
       " 52,\n",
       " 75,\n",
       " 2491,\n",
       " 83,\n",
       " 92,\n",
       " 111,\n",
       " 1563,\n",
       " 744,\n",
       " 68,\n",
       " 807,\n",
       " 94,\n",
       " 108,\n",
       " 69,\n",
       " 1093,\n",
       " 647,\n",
       " 65,\n",
       " 974,\n",
       " 502,\n",
       " 1022,\n",
       " 1246,\n",
       " 2432,\n",
       " 182,\n",
       " 855,\n",
       " 173,\n",
       " 751,\n",
       " 3350,\n",
       " 29,\n",
       " 3171,\n",
       " 331,\n",
       " 81,\n",
       " 193,\n",
       " 217,\n",
       " 421,\n",
       " 632,\n",
       " 2530,\n",
       " 680,\n",
       " 59,\n",
       " 1609,\n",
       " 3199,\n",
       " 37,\n",
       " 50,\n",
       " 47,\n",
       " 47,\n",
       " 296,\n",
       " 9,\n",
       " 51,\n",
       " 49,\n",
       " 53,\n",
       " 45,\n",
       " 85,\n",
       " 917,\n",
       " 52,\n",
       " 47,\n",
       " 1989,\n",
       " 859,\n",
       " 37,\n",
       " 98,\n",
       " 826,\n",
       " 2368,\n",
       " 52,\n",
       " 671,\n",
       " 1949,\n",
       " 502,\n",
       " 157,\n",
       " 277,\n",
       " 45,\n",
       " 681,\n",
       " 50,\n",
       " 73,\n",
       " 1675,\n",
       " 329,\n",
       " 63,\n",
       " 110,\n",
       " 39,\n",
       " 337,\n",
       " 86,\n",
       " 18,\n",
       " 3824,\n",
       " 48,\n",
       " 74,\n",
       " 151,\n",
       " 198,\n",
       " 910,\n",
       " 51,\n",
       " 58,\n",
       " 70,\n",
       " 773,\n",
       " 43,\n",
       " 62,\n",
       " 40,\n",
       " 17,\n",
       " 1090,\n",
       " 107,\n",
       " 42,\n",
       " 64,\n",
       " 82,\n",
       " 117,\n",
       " 104,\n",
       " 119,\n",
       " 77,\n",
       " 798,\n",
       " 105,\n",
       " 208,\n",
       " 2705,\n",
       " 79,\n",
       " 1017,\n",
       " 40,\n",
       " 830,\n",
       " 451,\n",
       " 2411,\n",
       " 790,\n",
       " 266,\n",
       " 140,\n",
       " 292,\n",
       " 66,\n",
       " 79,\n",
       " 49,\n",
       " 41,\n",
       " 742,\n",
       " 1055,\n",
       " 784,\n",
       " 71,\n",
       " 286,\n",
       " 90,\n",
       " 1088,\n",
       " 1024,\n",
       " 30,\n",
       " 1255,\n",
       " 553,\n",
       " 62,\n",
       " 1614,\n",
       " 313,\n",
       " 1032,\n",
       " 159,\n",
       " 1075,\n",
       " 921,\n",
       " 61,\n",
       " 2141,\n",
       " 60,\n",
       " 37,\n",
       " 1251,\n",
       " 61,\n",
       " 340,\n",
       " 3886,\n",
       " 565,\n",
       " 809,\n",
       " 92,\n",
       " 74,\n",
       " 64,\n",
       " 2099,\n",
       " 3885,\n",
       " 219,\n",
       " 55,\n",
       " 668,\n",
       " 149,\n",
       " 3390,\n",
       " 236,\n",
       " 225,\n",
       " 42,\n",
       " 501,\n",
       " 52,\n",
       " 1459,\n",
       " 967,\n",
       " 55,\n",
       " 290,\n",
       " 1370,\n",
       " 138,\n",
       " 301,\n",
       " 184,\n",
       " 773,\n",
       " 2388,\n",
       " 5537,\n",
       " 36,\n",
       " 62,\n",
       " 3117,\n",
       " 623,\n",
       " 344,\n",
       " 103,\n",
       " 68,\n",
       " 248,\n",
       " 3714,\n",
       " 55,\n",
       " 936,\n",
       " 754,\n",
       " 1544,\n",
       " 82,\n",
       " 134,\n",
       " 483,\n",
       " 1177,\n",
       " 73,\n",
       " 52,\n",
       " 50,\n",
       " 56,\n",
       " 193,\n",
       " 405,\n",
       " 39,\n",
       " 586,\n",
       " 522,\n",
       " 53,\n",
       " 94,\n",
       " 127,\n",
       " 1195,\n",
       " 1256,\n",
       " 185,\n",
       " 219,\n",
       " 70,\n",
       " 68,\n",
       " 51,\n",
       " 29,\n",
       " 107,\n",
       " 344,\n",
       " 263,\n",
       " 1149,\n",
       " 39,\n",
       " 49,\n",
       " 84,\n",
       " 1652,\n",
       " 47,\n",
       " 42,\n",
       " 64,\n",
       " 874,\n",
       " 126,\n",
       " 208,\n",
       " 43,\n",
       " 56,\n",
       " 40,\n",
       " 66,\n",
       " 102,\n",
       " 280,\n",
       " 155,\n",
       " 156,\n",
       " 101,\n",
       " 51,\n",
       " 132,\n",
       " 1082,\n",
       " 389,\n",
       " 75,\n",
       " 60,\n",
       " 357,\n",
       " 356,\n",
       " 66,\n",
       " 89,\n",
       " 2285,\n",
       " 139,\n",
       " 49,\n",
       " 1510,\n",
       " 84,\n",
       " 46,\n",
       " 57,\n",
       " 61,\n",
       " 347,\n",
       " 29,\n",
       " 55,\n",
       " 564,\n",
       " 1431,\n",
       " 56,\n",
       " 88,\n",
       " 479,\n",
       " 656,\n",
       " 92,\n",
       " 245,\n",
       " 44,\n",
       " 114,\n",
       " 57,\n",
       " 84,\n",
       " 579,\n",
       " 51,\n",
       " 950,\n",
       " 45,\n",
       " 842,\n",
       " 353,\n",
       " 659,\n",
       " 101,\n",
       " 2556,\n",
       " 44,\n",
       " 42,\n",
       " 50,\n",
       " 313,\n",
       " 85,\n",
       " 117,\n",
       " 232,\n",
       " 275,\n",
       " 509,\n",
       " 99,\n",
       " 250,\n",
       " 1698,\n",
       " 87,\n",
       " 54,\n",
       " 2823,\n",
       " 75,\n",
       " 34,\n",
       " 109,\n",
       " 73,\n",
       " 53,\n",
       " 97,\n",
       " 48,\n",
       " 34,\n",
       " 34,\n",
       " 76,\n",
       " 49,\n",
       " 53,\n",
       " 934,\n",
       " 458,\n",
       " 1333,\n",
       " 106,\n",
       " 625,\n",
       " 335,\n",
       " 1496,\n",
       " 358,\n",
       " 195,\n",
       " 952,\n",
       " 55,\n",
       " 145,\n",
       " 739,\n",
       " 75,\n",
       " 495,\n",
       " 80,\n",
       " 230,\n",
       " 35,\n",
       " 1020,\n",
       " 174,\n",
       " 55,\n",
       " 614,\n",
       " 985,\n",
       " 56,\n",
       " 125,\n",
       " 44,\n",
       " 204,\n",
       " 804,\n",
       " 187,\n",
       " 77,\n",
       " 672,\n",
       " 294,\n",
       " 74,\n",
       " 62,\n",
       " 707,\n",
       " 74,\n",
       " 100,\n",
       " 59,\n",
       " 1673,\n",
       " 60,\n",
       " 67,\n",
       " 51,\n",
       " 54,\n",
       " 57,\n",
       " 544,\n",
       " 1012,\n",
       " 1215,\n",
       " 486,\n",
       " 232,\n",
       " 485,\n",
       " 76,\n",
       " 54,\n",
       " 50,\n",
       " 241,\n",
       " 429,\n",
       " 56,\n",
       " 118,\n",
       " 72,\n",
       " 126,\n",
       " 64,\n",
       " 55,\n",
       " 52,\n",
       " 113,\n",
       " 53,\n",
       " 52,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=return_dataloader(X_train,y_0,params,is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_labels = batch[1]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids[0][:20,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 13:27:07.329630 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "I0924 13:27:08.513124 140120072415040 connectionpool.py:700] Starting new HTTPS connection (1): cdn.huggingface.co\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_path': 'bert-base-multilingual-cased', 'max_length': 128, 'batch_size': 6, 'weights': [1.0, 1.0], 'data_path': '../Data/New_Data_15-06-2020/Fearspeech_data_final.pkl', 'max_sentences_per_doc': 10, 'transformer_type': 'lstm_transformer', 'device': 'cuda', 'learning_rate': 2e-05, 'epsilon': 1e-08, 'random_seed': 2, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing DocumentBERTLSTM: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DocumentBERTLSTM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DocumentBERTLSTM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DocumentBERTLSTM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'classifier.1.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=select_transformer_model(params['transformer_type'],params['model_path'],params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (2): Tanh()\n",
       ")>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
